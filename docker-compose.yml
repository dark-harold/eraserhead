# üòê Podman Compose for vLLM and tinyclaw services
# Harold's distributed brain architecture
# Use: podman-compose up -d

version: '3.8'

services:
  # üòê Harold's GPU Brain (vLLM inference server)
  harold-brain-vllm:
    image: vllm/vllm-openai:latest
    container_name: harold-brain-vllm
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Model: Qwen2.5-Coder-7B-Instruct (selected for code generation)
      - MODEL_NAME=${VLLM_MODEL:-Qwen/Qwen2.5-Coder-7B-Instruct}
      - GPU_MEMORY_UTILIZATION=0.90
      - MAX_MODEL_LEN=8192
      - SERVED_MODEL_NAME=qwen2.5-coder-7b
    volumes:
      - ./models:/models:ro
      - vllm-cache:/root/.cache
    devices:
      - nvidia.com/gpu=all
    security_opt:
      - label=disable
    shm_size: '8gb'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - eraserhead-net
    # üòê Dark Harold's security paranoia
    labels:
      - "com.eraserhead.service=vllm"
      - "com.eraserhead.security=isolated"

  # üòê Harold's Memory (optional: separate tinyclaw gateway)
  # Uncomment if running tinyclaw as a service (currently uses local install)
  # harold-memory:
  #   image: node:22-alpine
  #   container_name: harold-memory
  #   restart: unless-stopped
  #   working_dir: /app
  #   command: npx tinyclaw serve
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - tinyclaw-config:/root/.config/tinyclaw:rw
  #     - ./TINYCLAW.md:/app/TINYCLAW.md:ro
  #   environment:
  #     - NODE_ENV=production
  #   networks:
  #     - eraserhead-net
  #   depends_on:
  #     - harold-brain-vllm

volumes:
  vllm-cache:
    driver: local
  tinyclaw-config:
    driver: local

networks:
  eraserhead-net:
    driver: bridge
    # üòê Harold's network isolation
    ipam:
      config:
        - subnet: 172.28.0.0/16
