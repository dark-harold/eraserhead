// üòê tinyclaw configuration
// Harold's memory and model routing (LOCAL-FIRST for privacy)
// Location: ~/.config/tinyclaw/config.json5

{
  agent: {
    // üòê Local-first: GPU (vllm) > CPU (llamacpp) > Cloud fallback
    provider: "auto",  // Auto-selects: vllm if GPU, llamacpp if CPU-only
    model: "qwen2.5-coder-7b-instruct",  // Harold's primary local brain
    thinkingLevel: "medium",
    fallbacks: [
      "qwen2.5-coder-3b-instruct",  // Local fallback (faster, less capable)
      "anthropic/claude-sonnet-4-5",  // Cloud fallback for complex tasks
      "anthropic/claude-opus-4-6"  // Last resort: maximum capability
    ],
    identity: {
      name: "Harold",
      emoji: "üòê"
    },
    responsePrefix: "",
    localFirst: true,  // üòê Privacy over convenience
    gpuAcceleration: true  // Use vLLM if CUDA/ROCm available
  },

  session: {
    resetMode: "idle",
    resetAtHour: 0,
    idleMinutes: 120
  },

  memory: {
    backend: "builtin",  // SQLite + FTS5 + sqlite-vec
    embeddingProvider: "openai",  // or "local" when available
    embeddingModel: "text-embedding-3-small"
  },

  // üòê Local model providers (privacy-first)
  models: {
    providers: {
      // vLLM for GPU (NVIDIA/AMD/Intel) - fastest for larger models
      vllm: {
        baseUrl: "http://127.0.0.1:8000/v1",
        api: "openai-completions",
        priority: 1,  // Prefer when GPU available
        requiresGpu: true,
        models: [
          {
            id: "qwen2.5-coder-7b-instruct",
            contextWindow: 32768,
            maxTokens: 8192,
            temperature: 0.7
          },
          {
            id: "qwen2.5-coder-3b-instruct",
            contextWindow: 32768,
            maxTokens: 8192,
            temperature: 0.7
          }
        ]
      },
      // llama.cpp for CPU (fallback) - works everywhere
      llamacpp: {
        baseUrl: "http://127.0.0.1:8080/v1",
        api: "openai-completions",
        priority: 2,  // Fallback when GPU unavailable
        requiresGpu: false,
        models: [
          {
            id: "qwen2.5-coder-3b-instruct",
            contextWindow: 32768,
            maxTokens: 8192,
            temperature: 0.7
          },
          {
            id: "qwen2.5-coder-1.5b-instruct",
            contextWindow: 32768,
            maxTokens: 8192,
            temperature: 0.7
          },
          {
            id: "qwen2.5-coder-0.5b-instruct",
            contextWindow: 32768,
            maxTokens: 4096,
            temperature: 0.7
          }
        ]
      },
      // Cloud providers (last resort)
      anthropic: {
        apiKey: "${ANTHROPIC_API_KEY}",
        priority: 99,  // Only when local fails or task requires Opus
        models: [
          {
            id: "claude-opus-4-6",
            contextWindow: 200000,
            maxTokens: 16384
          },
          {
            id: "claude-sonnet-4-5",
            contextWindow: 200000,
            maxTokens: 8192
          }
        ]
      }
    }
  },

  // üòê Dark Harold's security settings
  security: {
    toolPolicy: "auto",
    ssrfProtection: true,
    execApproval: "confirm",  // Require confirmation for shell execution
    pairingRequired: false,
    maxToolCallsPerTurn: 50
  },

  sandbox: {
    enabled: false,  // Enable for untrusted code execution
    image: "tinyclaw-sandbox",
    networkMode: "none"
  },

  // üòê Gateway configuration (optional remote access)
  gateway: {
    enabled: false,  // Set true to expose HTTP/WebSocket API
    port: 3000,
    bind: "127.0.0.1",  // Localhost only for security
    auth: {
      enabled: false
    }
  },

  pipeline: {
    inboundDebounceMs: 1500,
    typingIndicator: true,
    envelope: true,
    collectMode: false,
    chunkSize: {
      min: 800,
      max: 1200
    }
  },

  plugins: {
    enabled: true  // Load plugins from ~/.config/tinyclaw/plugins/
  }
}
