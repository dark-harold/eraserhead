models:
  haiku:
    id: "haiku"
    cost_per_1m: 0.25
    max_tokens: 50000
  sonnet:
    id: "sonnet"
    cost_per_1m: 3.00
    max_tokens: 200000
  opus:
    id: "opus"
    cost_per_1m: 15.00
    max_tokens: 1000000

  # üòê Local models (Harold's privacy-first brain)
  qwen2_3-coder-0_5b:
    id: "qwen2.3-coder-0.5b-instruct"
    cost_per_1m: 0.0  # Local = free (just electricity and Harold's suffering)
    max_tokens: 32768
    context_window: 32768
    quantization: "q4_k_m"
    size_gb: 0.4
    use_case: "Simple refactoring, formatting, quick fixes"
    provider: "llamacpp"
    
  qwen2_3-coder-1_5b:
    id: "qwen2.3-coder-1.5b-instruct"
    cost_per_1m: 0.0
    max_tokens: 32768
    context_window: 32768
    quantization: "q4_k_m"
    size_gb: 1.1
    use_case: "Testing, documentation, moderate complexity code"
    provider: "llamacpp"
    
  qwen2_3-coder-3b:
    id: "qwen2.3-coder-3b-instruct"
    cost_per_1m: 0.0
    max_tokens: 32768
    context_window: 32768
    quantization: "q4_k_m"
    size_gb: 2.2
    use_case: "Implementation, refactoring, code review"
    provider: "llamacpp"
    
  qwen2_3-coder-7b:
    id: "qwen2.3-coder-7b-instruct"
    cost_per_1m: 0.0
    max_tokens: 32768
    context_window: 32768
    quantization: "q4_k_m"
    size_gb: 4.9
    use_case: "Complex implementation, architecture, security analysis"
    provider: "llamacpp"  # or vllm when GPU available

# üòê Harold's model routing wisdom
# Local-first for privacy, cloud for complex reasoning
local_first:
  enabled: true  # Prefer local models when available
  gpu_model: "qwen2_3-coder-7b"  # vLLM when GPU detected
  cpu_model: "qwen2_3-coder-3b"  # llama.cpp fallback
  fallback_cloud: true  # Use cloud for truly complex tasks

task_patterns:
  simple:
    keywords:
      - "format"
      - "lint"
      - "typo"
      - "rename"
      - "simple"
      - "quick"
    model: qwen2_3-coder-0_5b  # Harold's fastest local brain
    fallback: haiku
  
  moderate:
    keywords:
      - "refactor"
      - "test"
      - "document"
      - "integrate"
      - "update"
      - "fix bug"
    model: qwen2_3-coder-3b  # Harold's capable local brain
    fallback: sonnet
  
  complex:
    keywords:
      - "architecture"
      - "design"
      - "protocol"
      - "system"
      - "opqwen2_3-coder-7b  # Harold's most powerful local brain
    fallback: timize"
    model: opus
    effort_level: medium

# üòê Dark Harold's security paranoia
force_patterns:
  always_opus:
    - "security audit"
    - "threat model"
    - "cryptography"
    - "encryption"
    - "anonymization"
    - "anemochory"
    - "vulnerability"
    - "attack"
    - "crypto"
    - "penetration test"
    - "packet routing"
    - "origin obfuscation"
    - "multi-layer"
    - "reverse engineer"
    - "exploit"
  
  extended_context:
    - "entire codebase"
    - "full analysis"
    - "comprehensive"
    - "all files"
    - "complete"

# üòê Language-specific routing
language_rules:
  cryptography:
    keywords:
      - "cryptography"
      - "encryption"
      - "hashing"
      - "key derivation"
    min_model: opus
  
  networking:
    keywords:
      - "socket"
      - "packet"
      - "network"
      - "routing"
      - "protocol"
    min_model: sonnet
  
  testing:
    keywords:
      - "pytest"
      - "test"
      - "mock"
      - "fixture"
    min_model: haiku

# Scoring thresholds (hardcoded in model_selector.py overrides these)
scoring:
  complexity_indicators:
    keywords_weight: 40
    context_size_weight: 20
    reasoning_depth_weight: 30
    code_analysis_weight: 10
  thresholds:
    haiku_max: 30
    sonnet_max: 70
    opus_min: 71

# üòê Effort levels for Opus tasks
effort_levels:
  low:
    cost_multiplier: 0.8
  medium:
    cost_multiplier: 1.0
  high:
    cost_multiplier: 1.2

context_windows:
  standard: 200000
  extended: 1000000

# Harold's output preferences
output:
  show_selection: true
  show_reasoning: false
  show_cost_estimate: false
  emoji: true
